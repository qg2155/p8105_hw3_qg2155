p8105\_hw3\_qg2155
================
Qiu Xia (Helen) Guan
10/11/2018

``` r
library(tidyverse)
```

    ## ── Attaching packages ────────────────────────── tidyverse 1.2.1 ──

    ## ✔ ggplot2 3.0.0     ✔ purrr   0.2.5
    ## ✔ tibble  1.4.2     ✔ dplyr   0.7.6
    ## ✔ tidyr   0.8.1     ✔ stringr 1.3.1
    ## ✔ readr   1.1.1     ✔ forcats 0.3.0

    ## ── Conflicts ───────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(patchwork)
library(p8105.datasets)
library(hexbin)
```

``` r
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 1,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

Problem 1
---------

### Load and clean BRFSS dataset

``` r
data("brfss_smart2010")
```

``` r
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, levels = str_c(c("Excellent" , "Very good", "Good", "Fair", "Poor"))))
```

### Summarizing distinct locations

``` r
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>%
  summarize(n_locations = n_distinct(locationdesc))
```

    ## # A tibble: 49 x 2
    ##    locationabbr n_locations
    ##    <chr>              <int>
    ##  1 AK                     1
    ##  2 AL                     1
    ##  3 AR                     1
    ##  4 AZ                     2
    ##  5 CA                     1
    ##  6 CO                     4
    ##  7 CT                     7
    ##  8 DC                     1
    ##  9 DE                     3
    ## 10 FL                     7
    ## # ... with 39 more rows

In 2002, there were 3 states that were observed at 7 locations. These 3 states were CT, FL, and NC.

### Creating spaghetti plot of number locations

``` r
brfss_data %>% 
  group_by(locationabbr, year) %>%
  summarize(n_locations = n_distinct(locationdesc)) %>% 
  ggplot(aes(x = year, y = n_locations, color = locationabbr)) + 
    geom_point() + geom_line()
```

<img src="p8105_hw3_qg2155_files/figure-markdown_github/spaghetti plot-1.png" width="90%" />

This spaghetti plot shows the number of locations in each state from 2002 to 2010. Florida in particular seems to have over 40 distinct locations in two different time points. Majority of states seem to have around 0 to 10 different locations across the years.

### Table for mean and standard deviation of "Excellent" responses in NY

``` r
brfss_data %>%
  group_by(year) %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  filter(locationabbr == "NY") %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable(digits = 1)
```

|  year|  mean\_excellent|  sd\_excellent|
|-----:|----------------:|--------------:|
|  2002|             24.0|            4.5|
|  2006|             22.5|            4.0|
|  2010|             22.7|            3.6|

Witin the state of New York, 2002 has the highest average of 24% for excellent responses, and a standard deviation of 4.5. Year 2006 has an average of 22.5% and standard deviation of 4.0. Year 2010 has an average of 22.7% and standard deviation of 3.6.

### Plot for distribution of each response category across states

``` r
brfss_data %>% 
  group_by(year, locationabbr, response) %>%
  summarize(mean_response = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_response)) +
    geom_violin() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    facet_grid(. ~ response) 
```

<img src="p8105_hw3_qg2155_files/figure-markdown_github/five plots-1.png" width="90%" />

Across all states and years, the response category of "Very good" seems to have the highest average, followed by "Good", "Excellent", "Fair", and "Poor". Average excellent responses range from 12 to almost 30%. Very good ranges from 16 to 43%. Good ranges from 24 to 36%. Fair ranges from 5 to 17%. Poor ranges from 2 to 9%. Very good responses seem to have the most variance whereas poor responses seem to have the least variance.

Problem 2
---------

### Load and clean dataset

``` r
data("instacart")
```

``` r
instacart_data = instacart %>% 
  janitor::clean_names() 
```

This instacart dataset has 15 variables and 1384617 observations. The variables are order\_id, product\_id, add\_to\_cart\_order, reordered, user\_id, eval\_set, order\_number, order\_dow, order\_hour\_of\_day, days\_since\_prior\_order, product\_name, aisle\_id, department\_id, aisle, and department. Out of all these variables, the key ones include order\_id, product\_id, reordered, order\_dow, order\_hour\_of\_day, days\_since\_prior\_order, product\_name, aisle, and department. These variables can give us a lot of information on popularity of different products, and customer behaviors (day and time or orderes, days in between orders).

Here is a demonstration of reading observation for customer with user\_id 1: This customer's order\_id is 1187899, and they ordered 11 items in this order. The products they ordered are added to cart in this order: soda, organic string cheese, 0% greek strained yogurt, XL paper towel rolls, milk chocolate almonds, postachios, cinnamon toast crunch, aged white cheddar popcorn, organic whole milk, organic half & half, and zero calorie cola. Most of these items belong in the snacks and dairy eggs departments. This ordered was placed on a Thursday and 8 AM. It has been 14 days since their prior order.

### Aisles and most items ordered

``` r
instacart_data %>% 
  group_by(aisle_id) %>% 
  summarize(n_most_ordered = n()) %>% 
  arrange(desc(n_most_ordered)) 
```

    ## # A tibble: 134 x 2
    ##    aisle_id n_most_ordered
    ##       <int>          <int>
    ##  1       83         150609
    ##  2       24         150473
    ##  3      123          78493
    ##  4      120          55240
    ##  5       21          41699
    ##  6      115          36617
    ##  7       84          32644
    ##  8      107          31269
    ##  9       91          26240
    ## 10      112          23635
    ## # ... with 124 more rows

There are 134 aisles. Aisle 83 has the most item ordered from because 150609 items were ordered from that aisle. This is followed by aisle 24 with 150473 items, and aisle 123 with 78493 items.

### Plot for number of items ordered in each aisle

``` r
instacart_data %>% 
  group_by(aisle_id) %>% 
  summarize(n_most_ordered = n()) %>%
  arrange(desc(n_most_ordered)) %>% 
  mutate(aisle_id = as.factor(aisle_id)) %>% 
  mutate(aisle_id = forcats::fct_reorder(aisle_id, n_most_ordered)) %>% 
  ggplot(aes(x = aisle_id, y = n_most_ordered)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Items ordered from aisle",
      x = "Aisle ID",
      y = "Count of items"
    ) + 
  scale_x_discrete(breaks = c("132", "68", "39", "60", "1", "89", "23", "4", "108", "78" , "83"))
```

<img src="p8105_hw3_qg2155_files/figure-markdown_github/aisle plot-1.png" width="90%" />

This bar graph shows that two aisles in particular have a much greater items ordered compared to the rest of the aisles. On the other end, there are also a good number of aisles that have barely any items ordered. A majority of the aisles get anywhere around 0 to 25000 items ordred.

### Table for most popular item in each of the three aisles

``` r
instacart_data %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  summarize(n_ordered = n()) %>% 
  arrange(desc(n_ordered)) %>%
  top_n(1) %>% 
  knitr::kable()
```

    ## Selecting by n_ordered

| aisle                      | product\_name                                 |  n\_ordered|
|:---------------------------|:----------------------------------------------|-----------:|
| packaged vegetables fruits | Organic Baby Spinach                          |        9784|
| baking ingredients         | Light Brown Sugar                             |         499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |          30|

The most popular product in the "packaged vegetables fruits" aisle is Organic Baby Spinach. It was ordered 9784 times. The most popular product in the "baking ingredients" aisle is Light Brown Sugar. It was ordered 499 times. The most popular product in the "dog food care" aisle is Snack Sticks Chicken & Rice Recipe Dog Treats. It was ordered 30 times.

### Table for Pink Lady Apples and Coffee Ice Cream Order Times

``` r
instacart_data %>% 
  select(order_id, product_id, order_dow, order_hour_of_day, product_name, aisle_id) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_order_hour) %>% 
  knitr::kable(digits = 2)
```

| product\_name    |      0|      1|      2|      3|      4|      5|      6|
|:-----------------|------:|------:|------:|------:|------:|------:|------:|
| Coffee Ice Cream |  13.77|  14.32|  15.38|  15.32|  15.22|  12.26|  13.83|
| Pink Lady Apples |  13.44|  11.36|  11.70|  14.25|  11.55|  12.78|  11.94|

The average order time for Coffee Ice Cream ranges from 12:26 to 15:38 throughout the week. The day with the earliest average order time is Friday and the day with the latest average order time is Tuesday. Pink Lady Apples have an overall earlier average order time than Coffee Ice Cream. The range for Pink Lady Apples average order times are 11:36 to 14:25. The day with the earliest average order time is Monday and the day with the latest average order time is Wednesday.

Problem 3
---------

### Load and clean dataset

``` r
data("ny_noaa")
```

``` r
noaa_data = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "date"), sep = "-") %>% 
  mutate(prcp = prcp/10,
         tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10
         ) 
```

Cleaned variable names and separated date into year, month, and date columns. Transformed the precipitation values from tenths of mm to mm by dividing original values by 10. Same is done to transform the minimum and maximum temperatures from tenths of Celsius to Celsius.

### To find the proportion of missing values

``` r
noaa_data %>% 
  summarize(n_missing_prcp = mean(is.na(prcp)),
            n_missing_snow = mean(is.na(snow)),
            n_missing_snwd = mean(is.na(snwd)),
            n_missing_tmax = mean(is.na(tmax)),
            n_missing_tmin = mean(is.na(tmin))
            )
```

    ## # A tibble: 1 x 5
    ##   n_missing_prcp n_missing_snow n_missing_snwd n_missing_tmax
    ##            <dbl>          <dbl>          <dbl>          <dbl>
    ## 1         0.0562          0.147          0.228          0.437
    ## # ... with 1 more variable: n_missing_tmin <dbl>

This NY NOAA dataset has 9 variables and 2595176 observations. The original 7 variables are id, date, prcp, snow, snwd, tmax, and tmin. After separting date into year, month, and day, there are now an additional 2 variables to make a total of 9 variables in the cleaned dataset. The key variables are date, precipitation, snow, snow depth, maximum and minimum temperatures. These variables will be able to give us a lot of information on the weather pattern in New York, especially for precipitation and snow in the winters.

One problem with this dataset is that there are many missing values for snow, snwd, tmax and tmin. This is probably due to the fact that some stations only collect a subset of variables. The proportion of missing in precipitation column is 5.62%, snow is 14.7%, snow depth is 22.8%, tmax and tmin are both 43.7%. The extent in which data is missing is problematic for our analysis. It will be difficult to get accurate weather patterns across NY since we might be missing critical pieces of information from stations that may be situated in locations that are in particular warmer or colder than average.

``` r
noaa_data %>% 
  group_by(snow) %>% 
  summarize(n_snow = n()) %>% 
  arrange(desc(n_snow))
```

    ## # A tibble: 282 x 2
    ##     snow  n_snow
    ##    <int>   <int>
    ##  1     0 2008508
    ##  2    NA  381221
    ##  3    25   31022
    ##  4    13   23095
    ##  5    51   18274
    ##  6    76   10173
    ##  7     8    9962
    ##  8     5    9748
    ##  9    38    9197
    ## 10     3    8790
    ## # ... with 272 more rows

The most commonly observed value for snowfall is 0 mm, with 2008508 observations. This is the expected value because it does not snow on most days throughout the years. The second commonly observed value is NA because this information is missing for many observations. The third commonly observed value is 25 mm with 31022 observations. The fourth commonly observed value is 13 mm with 23095 observations. These values tell us that when it does snow in NY, it is usually very light.

### Plot of average max temp in Jan and July

``` r
noaa_data %>% 
  filter(month %in% c("01", "07")) %>% 
  group_by(year, month, date) %>%
  summarize(tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = tmax_mean)) +
    geom_boxplot() +
    facet_grid(. ~month) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    labs(
      title = "Average max temperature in Janurary and July across years",
      x = "Years",
      y = "Average max temperature (C)"
    )
```

<img src="p8105_hw3_qg2155_files/figure-markdown_github/avg max-1.png" width="90%" />

The graphs show that there is an appreciable difference between the average maximum temperatures between Jan and July across all years. The range for the average max temp in Jan is anywhere from -15 C to 15 C whereas the average for July is between 20 C to 34 C. There are outliers in some of the years for both months. July has more outliers for low temperatures across years whereas Jan has a relatively equal number of outliers for warmer and cooler average maximum temperatures. Year 1988 had an outlier for especially low temperature (17C) in July. The average maximum temperatures in July also have a smaller variance compared to Janurary judging by the size of the box plots.

### two panel plot for temperatures and snowfall

``` r
tmin_tmax_plot = ggplot(noaa_data, aes(x = tmin, y = tmax)) +
  geom_hex() +
  labs(
    title = "Temperature Plot",
    x = "Minimum daily temperature (C)",
    y = "Maximum daily temperature (C)"
  )

snowfall_plot = noaa_data %>% 
  filter(snow > 0 & snow < 100) %>% 
  group_by(year) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
  labs(
    title = "Distribution of Snowfall(0>snow<100) by Year",
    x = "Year",
    y = "Distribution of Snow"
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 

tmin_tmax_plot/snowfall_plot
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

<img src="p8105_hw3_qg2155_files/figure-markdown_github/two panel plot-1.png" width="90%" />

The first tmax and tmin plot shows that the minimum daily temperatures can range from -45 C to 40 C, with outliers near -60 C on the cold end and up to 60 C on the warm end. On the other end, maximum temperatures can range from -30 C to 40 C, with outliers as cold as -40 C and as hot as 60 C. The light shade of blue in the center of the cluster represents the range for the most common maximum and minimum temperatures. This means the most observed minimum temperature in NY is from 0 C to 15 C, and the most observed maximum temperature is from 10 C to 30 C.

The second box plot shows the distribution of snowfall across all years. This box plot shows a rather stable distribution of snowfall across most years because the boxes are very similar to one another. The average distribution of snowfall is around 25 mm across all years. Years 1998, 2004, 2006, 2007, and 2010 had noticibly less snowfalls than usual. However, year 2006 also had many outliers for high snall fall ranging up to 100 mm. Years 1998 and 2010 also had a few outliers for heavy snowfall.
