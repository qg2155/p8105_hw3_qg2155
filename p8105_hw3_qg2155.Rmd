---
title: "p8105_hw3_qg2155"
author: "Qiu Xia (Helen) Guan"
date: "10/11/2018"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(tidyverse)
library(patchwork)
library(p8105.datasets)
```

```{r page setup}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 1.3,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Problem 1

### Load and clean BRFSS dataset 

```{r get brfss}
data("brfss_smart2010")
```

```{r clean brfss}
brfss_data = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = as.factor(response))
```

### Summarizing distinct locations 

```{r states}
brfss_data %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>%
  summarize(n_locations = n_distinct(locationdesc))
```

In 2002, there were 3 states that were observed at 7 locations. These 3 states were CT, FL, and NC. 

### Creating spaghetti plot of number locations 

```{r spaghetti plot}
brfss_data %>% 
  group_by(locationabbr, year) %>%
  summarize(n_locations = n_distinct(locationdesc)) %>% 
  ggplot(aes(x = year, y = n_locations, color = locationabbr)) + 
    geom_point() + geom_line()
```

This spaghetti plot shows the number of locations in each state from 2002 to 2010. Florida in particular seems to have over 40 distinct loactions in two different time points. 

### Table for mean and standard deviation of "Excellent" responses in NY

```{r  excellent responses}
brfss_data %>%
  group_by(year) %>% 
  filter(year == 2002 | year == 2006 | year == 2010) %>% 
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>% 
  filter(locationabbr == "NY") %>% 
  summarize(mean_excellent = mean(excellent, na.rm = TRUE),
            sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable(digits = 1)
```

Witin the state of New York, 2002 has the highest average of 24% for excellent responses, and a standard deviation of 4.5. Year 2006 has an average of 22.5% and standard deviation of 4.0. Year 2010 has an average of 22.7% and standard deviation of 3.6. 

### Plot for distribution of each response category across states

```{r five plots}
brfss_data %>% 
  group_by(year, locationabbr, response) %>%
  summarize(mean_response = mean(data_value, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = mean_response)) +
    geom_violin() +
    facet_grid(. ~ response) 
```

Across all states and years, the response category of "Good" seems to have the highest average, followed by "Excellent", "Fair", and "Poor". Excellent seems to have the biggest range out of them all. 

## Problem 2

### Load and clean dataset

```{r get instacart}
data("instacart")
```

```{r clean instacart}
instacart_data = instacart %>% 
  janitor::clean_names() 
```

This instacart dataset has `r ncol(instacart_data)` variables and `r nrow(instacart_data)` observations. The variables are order_id, product_id, add_to_cart_order, reordered, user_id, eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order, product_name, aisle_id, department_id, aisle, and department. Out of all these variables, the key ones include order_id, product_id, reordered, order_dow, order_hour_of_day, days_since_prior_order, product_name, aisle, and department. These variables can give us a lot of information on popularity of different products, and customer behaviors (day and time or orderes, days in between orders). 

Here is a demonstration of reading observation for customer with user_id 1:
This customer's order_id is 1187899, and they ordered 11 items in this order. The products they ordered are added to cart in this order: soda, organic string cheese, 0% greek strained yogurt, XL paper towel rolls, milk chocolate almonds, postachios, cinnamon toast crunch, aged white cheddar popcorn, organic whole milk, organic half & half, and zero calorie cola. Most of these items belong in the snacks and dairy eggs departments. This ordered was placed on a Thursday and 8 AM. It has been 14 days since their prior order. 

### Aisles and most items ordered 

```{r most items}
instacart_data %>% 
  group_by(aisle_id) %>% 
  summarize(n_most_ordered = n()) %>% 
  arrange(desc(n_most_ordered)) 
```

There are `r instacart_data %>% summarize(n_aisles = n_distinct(aisle_id))` aisles. Aisle 83 has the most item ordered from because 150609 items were ordered from that aisle. This is followed by aisle 24 with 150473 items, and aisle 123 with 78493 items. 

### Plot for number of items ordered in each aisle

```{r aisle plot}
instacart_data %>% 
  group_by(aisle_id) %>% 
  summarize(n_most_ordered = n()) %>%
  arrange(desc(n_most_ordered)) %>% 
  mutate(aisle_id = as.factor(aisle_id)) %>% 
  mutate(aisle_id = forcats::fct_reorder(aisle_id, n_most_ordered)) %>% 
  ggplot(aes(x = aisle_id, y = n_most_ordered)) +
    geom_bar(stat = "identity") +
    labs(
      title = "Items ordered from aisle",
      x = "Aisle ID",
      y = "Count of items"
    ) + 
  scale_x_discrete(breaks = c("132", "68", "39", "60", "1", "89", "23", "4", "108", "78" , "83"))

```

### Table for most popular item in each of the three aisles

```{r popular item}
instacart_data %>% 
  group_by(aisle, product_name) %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  summarize(n_ordered = n()) %>% 
  arrange(desc(n_ordered)) %>%
  top_n(1) %>% 
  knitr::kable()
```

The most popular product in the "packaged vegetables fruits" aisle is Organic Baby Spinach. It was ordered 9784 times. The most popular product in the "baking ingredients" aisle is Light Brown Sugar. It was ordered 499 times. The most popular product in the "dog food care" aisle is Snack Sticks Chicken & Rice Recipe Dog Treats. It was ordered 30 times. 

### Table for Pink Lady Apples and Coffee Ice Cream Order Times

```{r}
instacart_data %>% 
  select(order_id, product_id, order_dow, order_hour_of_day, product_name, aisle_id) %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_order_hour) %>% 
  knitr::kable(digits = 2)
```

The average order time for Coffee Ice Cream ranges from 12:26 to 15:38 throughout the week. The day with the earliest average order time is Friday and the day with the latest average order time is Tuesday. Pink Lady Apples have an overall earlier average order time than Coffee Ice Cream. The range for Pink Lady Apples average order times are 11:36 to 14:25. The day with the earliest average order time is Monday and the day with the latest average order time is Wednesday. 

## Problem 3

### Load and clean dataset

```{r get noaa}
data("ny_noaa")
```

```{r clean noaa}
noaa_data = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "date"), sep = "-") %>% 
  mutate(prcp = prcp/10,
         tmax = as.numeric(tmax)/10,
         tmin = as.numeric(tmin)/10
         ) 
```
Cleaned variable names and separated date into year, month, and date columns. Transformed the precipitation values from tenths of mm to mm by dividing original values by 10. Same is done to transform the minimum and maximum temperatures from tenths of Celsius to Celsius. 

### To find the proportion of missing values
```{r missing values}
noaa_data %>% 
  summarize(n_missing_prcp = mean(is.na(prcp)),
            n_missing_snow = mean(is.na(snow)),
            n_missing_snwd = mean(is.na(snwd)),
            n_missing_tmax = mean(is.na(tmax)),
            n_missing_tmin = mean(is.na(tmin))
            )
```

This NY NOAA dataset has `r ncol(noaa_data)` variables and `r nrow(noaa_data)` observations. The variables are id, date, prcp, snow, snwd, tmax, and tmin. The key variables are date, precipitation, snow, snow depth, maximum and minimum temperatures. These variables will be able to give us a lot of information on the weather pattern in New York, especially for precipitation and snow in the winters. One problem with this dataset is that there are many missing values for snow, snwd, tmax and tmin. The proportion of missing in precipitation column is 5.62%, snow is 14.7%, snow depth is 22.8%, tmax and tmin are both 43.7%. This extent in which data is missing is problematic for our analysis.

```{r snowfall}
noaa_data %>% 
  group_by(snow) %>% 
  summarize(n_snow = n()) %>% 
  arrange(desc(n_snow))
```

The most commonly observed value for snowfall is 0 mm, with 2008508 observations. This is the expected value because it does not snow on most days throughout the years. The second commonly observed value is NA because this information is missing for many observations. The third commonly observed value is 25 mm with 31022 observations. The fourth commonly observed value is 13 mm with 23095 observations. These values tell us that when it does snow in NY, it is usually very light. 

### Plot of average max temp in Jan and July 

```{r avg max}
noaa_data %>% 
  filter(month %in% c("01", "07")) %>% 
  group_by(year, month, date) %>%
  summarize(tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = tmax_mean)) +
    geom_boxplot() +
    facet_grid(. ~month) +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    labs(
      title = "Average max temperature in Janurary and July across years",
      x = "Years",
      y = "Average max temperature (C)"
    )
```

The graphs show that there is an appreciable difference between the average maximum temperatures between Jan and July across all years. The range for the average max temp in Jan is anywhere from -15 C to 15 C whereas the average for July is between 20 C to 34 C. There are outliers in some of the years for both months. July has more outliers for low temperatures across years whereas Jan has a relatively equal number of outliers for warmer and cooler average maximum temperatures. Year 1988 had an outlier for especially low temperature (17C) in July. The average maximum temperatures in July also have a smaller variance compared to Janurary judging by the size of the box plots. 

### two panel plot for temperatures and snowfall
```{r download hexbin}
install.packages("hexbin")
library(hexbin)
```

```{r two panel plot}
noaa_data %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_point()
```


